# CSV to JSON Converter

A robust, production-ready Python application for converting retail database CSV files to JSON format with comprehensive data validation and transformation capabilities.

## ğŸ—ï¸ Architecture

This project follows data engineering best practices with a modular, component-based architecture:

```
src/
â”œâ”€â”€ utils/          # Configuration and logging utilities
â”œâ”€â”€ readers/        # CSV data reading components  
â”œâ”€â”€ processors/     # Data validation and transformation
â”œâ”€â”€ writers/        # JSON output generation
â””â”€â”€ main.py         # Main orchestration pipeline
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Virtual environment support

### Installation

1. **Setup Virtual Environment**
   ```bash
   python -m venv cd-venv
   source cd-venv/bin/activate  # On Windows: cd-venv\Scripts\activate
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify Data Structure**
   Ensure your retail database CSV files are in the expected structure:
   ```
   retail_db/
   â”œâ”€â”€ departments/part-00000
   â”œâ”€â”€ categories/part-00000
   â”œâ”€â”€ orders/part-00000
   â”œâ”€â”€ products/part-00000
   â”œâ”€â”€ customers/part-00000
   â””â”€â”€ order_items/part-00000
   ```

### Usage

#### Option 1: Run Complete Conversion
```bash
python -m src.main
```

#### Option 2: Test the Pipeline
```bash
python test_conversion.py
```

#### Option 3: Use as a Module
```python
from src.main import CSVToJSONConverter

converter = CSVToJSONConverter()
results = converter.run_conversion()
converter.print_final_report(results)
```

## ğŸ“Š Data Processing Pipeline

The conversion follows a 5-phase approach:

### Phase 1: Configuration
- Loads schemas and settings from YAML files
- Validates configuration integrity
- Prepares output directories

### Phase 2: CSV Reading
- Reads CSV files with schema-based data types
- Handles missing values and data type conversions
- Provides detailed read statistics

### Phase 3: Data Validation
- Validates data integrity and completeness
- Checks required fields and data types
- Generates comprehensive quality reports

### Phase 4: Data Transformation  
- Transforms data to JSON-ready format
- Handles pandas-specific types and null values
- Generates metadata for each table

### Phase 5: JSON Writing
- Writes individual table JSON files
- Creates combined dataset file
- Validates output file integrity

## ğŸ“ Output Structure

The converter generates:

### Individual Files
```
data/output/
â”œâ”€â”€ departments.json
â”œâ”€â”€ categories.json
â”œâ”€â”€ orders.json
â”œâ”€â”€ products.json
â”œâ”€â”€ customers.json
â””â”€â”€ order_items.json
```

### Combined File
```
data/output/retail_db_combined.json
```

### JSON Format
Each file contains:
```json
{
  "metadata": {
    "table_name": "departments",
    "record_count": 6,
    "generated_at": "2024-12-19T...",
    "data_types": {...},
    "statistics": {...}
  },
  "data": [
    {"department_id": 1, "department_name": "Management"},
    ...
  ]
}
```

## âš™ï¸ Configuration

### Schema Definition (`config/schemas.yaml`)
Defines table structures, data types, and validation rules:
```yaml
tables:
  departments:
    columns:
      department_id:
        type: "int64"
        required: true
        position: 1
      department_name:
        type: "string"
        required: true
        position: 2
```

### Application Settings (`config/settings.yaml`)
Controls processing behavior:
```yaml
input:
  base_path: "data/input/retail_db"
  file_pattern: "part-00000"

output:
  base_path: "data/output"
  individual_files: true
  combined_file: true

processing:
  validate_data: true
  chunk_size: 10000
```

## ğŸ”§ Components

### Utils
- **ConfigManager**: YAML configuration loading and management
- **Logger**: Centralized logging with file/console output

### Readers  
- **SchemaReader**: Schema parsing and pandas dtype generation
- **CSVReader**: Schema-aware CSV reading with proper data types

### Processors
- **DataValidator**: Comprehensive data quality validation
- **DataTransformer**: JSON-ready data transformation

### Writers
- **JSONWriter**: Formatted JSON output with error handling

## ğŸ“‹ Features

### Data Quality
- âœ… Schema-based validation
- âœ… Missing value handling
- âœ… Data type verification
- âœ… Quality scoring system

### Performance
- âœ… Efficient pandas operations
- âœ… Memory-conscious processing
- âœ… Detailed performance metrics

### Reliability
- âœ… Comprehensive error handling
- âœ… Detailed logging
- âœ… Output validation
- âœ… Graceful failure handling

### Flexibility
- âœ… Configuration-driven processing
- âœ… Modular component architecture
- âœ… Optional validation steps
- âœ… Custom output paths

## ğŸ“ˆ Sample Output

```
================================================================================
CSV TO JSON CONVERSION - FINAL REPORT
================================================================================
Status: SUCCESS
Duration: 2.45 seconds
Tables Processed: 6
Total Rows: 255,379
Files Written: 6
Combined File: Yes

Output Directory: data/output

Individual Files:
  â€¢ departments: data/output/departments.json
  â€¢ categories: data/output/categories.json
  â€¢ orders: data/output/orders.json
  â€¢ products: data/output/products.json
  â€¢ customers: data/output/customers.json
  â€¢ order_items: data/output/order_items.json

Combined File: data/output/retail_db_combined.json
================================================================================
```

## ğŸ§ª Testing

Run comprehensive tests:
```bash
python -m pytest tests/ -v
```

Individual component testing:
```bash
# Test configuration loading
python -c "from src.utils.config import config_manager; print(config_manager.load_settings())"

# Test schema reading  
python -c "from src.readers.schema_reader import schema_reader; print(schema_reader.get_available_tables())"

# Test CSV reading
python -c "from src.readers.csv_reader import csv_reader; df = csv_reader.read_csv_file('departments'); print(df.info())"
```

## ğŸš¦ Error Handling

The application provides detailed error information:
- Configuration errors with specific file/line details
- Data validation issues with affected records
- File I/O problems with permissions and paths
- Transformation errors with data type conflicts

## ğŸ“ Logging

Logs are written to:
- **Console**: Real-time progress and status
- **File**: `logs/conversion.log` with detailed operations

Log levels: DEBUG, INFO, WARNING, ERROR

## ğŸ”„ Workflow Integration

The converter can be integrated into data pipelines:
```python
# Programmatic usage
converter = CSVToJSONConverter()
results = converter.run_conversion(validate_data=False)

if results['success']:
    print(f"Converted {results['summary']['total_rows_read']} rows")
    # Continue with downstream processing
```

## ğŸ› ï¸ Development

### Project Structure
```
â”œâ”€â”€ config/           # Configuration files
â”œâ”€â”€ data/            # Input/output data directories
â”œâ”€â”€ logs/            # Application logs
â”œâ”€â”€ src/             # Source code
â”œâ”€â”€ tests/           # Test files
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md       # This file
```

### Design Principles
- **Separation of Concerns**: Each component has a single responsibility
- **Configuration-Driven**: Behavior controlled via YAML files
- **Error Resilience**: Graceful handling of failures
- **Observability**: Comprehensive logging and metrics

## ğŸ“‹ License

This project is designed for educational and internal use as part of data engineering best practices demonstration.
